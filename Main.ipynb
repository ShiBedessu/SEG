{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527e238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "353ade9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1Ô∏è‚É£ L√†m s·∫°ch vƒÉn b·∫£n\n",
    "# ===============================\n",
    "def clean_formatting(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.replace('\\r', '\\n')\n",
    "    text = re.sub(r\"(?m)^[\\*\\=\\-‚Äì_\\.]{3,}\\s*$\", \"\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\", text)\n",
    "    text = re.sub(r\"([^\\n])\\n([^\\n])\", r\"\\1 \\2\", text)\n",
    "    text = re.sub(r\"KH√îNGS·ªê\", \"KH√îNG S·ªê\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ Suy lu·∫≠n lƒ©nh v·ª±c (d·ª± ph√≤ng)\n",
    "# ===============================\n",
    "def infer_field_category(title: str | None, agency: str | None, content: str | None) -> str | None:\n",
    "    combined = \" \".join(filter(None, [title, agency, content])).lower()\n",
    "    domain_keywords = {\n",
    "        \"gi√°o d·ª•c\": [\"gi√°o d·ª•c\", \"tr∆∞·ªùng h·ªçc\", \"h·ªçc sinh\", \"ƒë·∫°i h·ªçc\", \"b·ªô gi√°o d·ª•c\"],\n",
    "        \"y t·∫ø\": [\"y t·∫ø\", \"b·ªánh vi·ªán\", \"s·ª©c kh·ªèe\", \"b·ªô y t·∫ø\"],\n",
    "        \"t√†i ch√≠nh\": [\"t√†i ch√≠nh\", \"ng√¢n s√°ch\", \"thu·∫ø\", \"kho b·∫°c\", \"b·ªô t√†i ch√≠nh\"],\n",
    "        \"ƒë·∫•t ƒëai\": [\"ƒë·∫•t ƒëai\", \"b·∫•t ƒë·ªông s·∫£n\", \"x√¢y d·ª±ng\", \"b·ªô x√¢y d·ª±ng\"],\n",
    "        \"lao ƒë·ªông\": [\"lao ƒë·ªông\", \"vi·ªác l√†m\", \"ti·ªÅn l∆∞∆°ng\", \"b·ªô lao ƒë·ªông\"],\n",
    "        \"n√¥ng nghi·ªáp\": [\"n√¥ng nghi·ªáp\", \"th·ªßy s·∫£n\", \"chƒÉn nu√¥i\", \"b·ªô n√¥ng nghi·ªáp\"],\n",
    "        \"giao th√¥ng\": [\"giao th√¥ng\", \"v·∫≠n t·∫£i\", \"ƒë∆∞·ªùng b·ªô\", \"b·ªô giao th√¥ng\"],\n",
    "        \"c√¥ng th∆∞∆°ng\": [\"c√¥ng th∆∞∆°ng\", \"th∆∞∆°ng m·∫°i\", \"xu·∫•t nh·∫≠p kh·∫©u\", \"b·ªô c√¥ng th∆∞∆°ng\"],\n",
    "        \"qu·ªëc ph√≤ng\": [\"qu·ªëc ph√≤ng\", \"qu√¢n ƒë·ªôi\", \"b·ªô qu·ªëc ph√≤ng\"],\n",
    "        \"c√¥ng an\": [\"c√¥ng an\", \"an ninh\", \"b·ªô c√¥ng an\"],\n",
    "        \"vƒÉn h√≥a\": [\"vƒÉn h√≥a\", \"th·ªÉ thao\", \"du l·ªãch\", \"b·ªô vƒÉn h√≥a\"],\n",
    "        \"m√¥i tr∆∞·ªùng\": [\"m√¥i tr∆∞·ªùng\", \"t√†i nguy√™n\", \"b·ªô t√†i nguy√™n\"],\n",
    "    }\n",
    "    for field, keywords in domain_keywords.items():\n",
    "        if any(kw in combined for kw in keywords):\n",
    "            return field.capitalize()\n",
    "    return \"Kh√°c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593c126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4Ô∏è‚É£ Chia vƒÉn b·∫£n th√†nh c√°c chunk\n",
    "# ===============================\n",
    "def chunk_document(content: str, max_chunk_size: int = 1000, overlap_words: int = 50) -> List[str]:\n",
    "    \"\"\"Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè d·ª±a tr√™n c√¢u, ƒëi·ªÅu lu·∫≠t, ch∆∞∆°ng,...\"\"\"\n",
    "    content = clean_formatting(content)\n",
    "\n",
    "    # --- T√°ch theo c√°c ch·ªâ m·ª•c ph√°p l√Ω (ƒêi·ªÅu, Kho·∫£n, Ch∆∞∆°ng, M·ª•c)\n",
    "    parts = re.split(r'(?=(ƒêi·ªÅu\\s+th·ª©|KHO·∫¢N\\s+TH·ª®|CH∆Ø∆†NG\\s+|M·ª§C\\s+))', content, flags=re.IGNORECASE)\n",
    "    if len(parts) == 1:\n",
    "        # N·∫øu kh√¥ng c√≥ 'ƒêi·ªÅu...', chia theo c√¢u\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "        parts = []\n",
    "        current = \"\"\n",
    "        for s in sentences:\n",
    "            if len(current) + len(s) < max_chunk_size:\n",
    "                current += \" \" + s\n",
    "            else:\n",
    "                parts.append(current.strip())\n",
    "                current = s\n",
    "        if current:\n",
    "            parts.append(current.strip())\n",
    "    else:\n",
    "        merged = []\n",
    "        for i in range(0, len(parts), 2):\n",
    "            section = \"\".join(parts[i:i+2]).strip()\n",
    "            if section:\n",
    "                merged.append(section)\n",
    "        parts = merged\n",
    "\n",
    "    # --- G·ªôp nh·ªè n·∫øu ƒëo·∫°n qu√° ng·∫Øn\n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "    for p in parts:\n",
    "        if len(buffer) + len(p) < max_chunk_size:\n",
    "            buffer += \" \" + p\n",
    "        else:\n",
    "            chunks.append(buffer.strip())\n",
    "            buffer = p\n",
    "    if buffer:\n",
    "        chunks.append(buffer.strip())\n",
    "\n",
    "    # --- Overlap gi·ªØa c√°c chunk\n",
    "    if overlap_words > 0 and len(chunks) > 1:\n",
    "        result = [chunks[0]]\n",
    "        for i in range(1, len(chunks)):\n",
    "            prev = chunks[i-1].split()\n",
    "            overlap = \" \".join(prev[-overlap_words:]) if len(prev) > overlap_words else chunks[i-1]\n",
    "            result.append((overlap + \" \" + chunks[i]).strip())\n",
    "        chunks = result\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Chu·∫©n h√≥a t·ª´ng record\n",
    "# ===============================\n",
    "def normalize_record(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    content = doc.get(\"mt6_text\") or doc.get(\"content\") or \"\"\n",
    "    content = clean_formatting(content)\n",
    "    law_id = str(doc.get(\"law_id\") or doc.get(\"lawid\") or \"unknown\")\n",
    "    return {\n",
    "        \"law_id\": law_id,\n",
    "        \"raw_title\": doc.get(\"title\"),\n",
    "        \"source_url\": doc.get(\"href\"),\n",
    "        \"category\": doc.get(\"category\"),\n",
    "        \"content\": content,\n",
    "        \"provided_issue_date\": doc.get(\"date\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a8b954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3Ô∏è‚É£ Tr√≠ch xu·∫•t metadata\n",
    "# ===============================\n",
    "def extract_metadata(content: str, law_id: str, title: str | None = None) -> Dict[str, Any]:\n",
    "    metadata = {\n",
    "        \"law_id\": law_id,\n",
    "        \"document_type\": None,\n",
    "        \"issuing_agency\": None,\n",
    "        \"issue_date\": None,\n",
    "        \"title\": None,\n",
    "    }\n",
    "\n",
    "    DOC_TYPES = [\n",
    "        \"C√îNG ƒêI·ªÜN\", \"NGH·ªä ƒê·ªäNH\", \"TH√îNG T∆Ø\", \"QUY·∫æT ƒê·ªäNH\", \"LU·∫¨T\",\n",
    "        \"NGH·ªä QUY·∫æT\", \"CH·ªà TH·ªä\", \"TH√îNG B√ÅO\", \"K·∫æ HO·∫†CH\", \"B√ÅO C√ÅO\",\n",
    "        \"VƒÇN B·∫¢N\", \"C√îNG VƒÇN\", \"S·∫ÆC L·ªÜNH\"\n",
    "    ]\n",
    "    AGENCIES = [\n",
    "        \"CH·ª¶ T·ªäCH\", \"CH√çNH PH·ª¶\", \"B·ªò\", \"QU·ªêC H·ªòI\", \"TH·ª¶ T∆Ø·ªöNG\",\n",
    "        \"T√íA √ÅN\", \"VI·ªÜN KI·ªÇM S√ÅT\", \"KI·ªÇM TO√ÅN\", \"UBND\", \"HƒêND\"\n",
    "    ]\n",
    "\n",
    "    def _norm(s: str) -> str:\n",
    "        s = unicodedata.normalize(\"NFC\", s or \"\").lower()\n",
    "        return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # 1Ô∏è‚É£ Lo·∫°i vƒÉn b·∫£n\n",
    "    if title:\n",
    "        tnorm = _norm(title)\n",
    "        for dt in DOC_TYPES:\n",
    "            if dt.lower() in tnorm:\n",
    "                metadata[\"document_type\"] = dt\n",
    "                break\n",
    "    if not metadata[\"document_type\"]:\n",
    "        head = \"\\n\".join(content.splitlines()[:20]).upper()\n",
    "        for dt in DOC_TYPES:\n",
    "            if re.search(rf\"\\b{dt}\\b\", head, re.IGNORECASE):\n",
    "                metadata[\"document_type\"] = dt\n",
    "                break\n",
    "\n",
    "    # 2Ô∏è‚É£ C∆° quan ban h√†nh\n",
    "    if title:\n",
    "        tnorm = _norm(title)\n",
    "        if \"ch·ªß t·ªãch\" in tnorm:\n",
    "            metadata[\"issuing_agency\"] = \"CH·ª¶ T·ªäCH N∆Ø·ªöC\"\n",
    "        elif \"b·ªô\" in tnorm:\n",
    "            m = re.search(r\"b·ªô\\s+([^\\d,.;]+)\", tnorm)\n",
    "            if m:\n",
    "                metadata[\"issuing_agency\"] = \"B·ªò \" + m.group(1).upper()\n",
    "        elif \"ch√≠nh ph·ªß\" in tnorm:\n",
    "            metadata[\"issuing_agency\"] = \"CH√çNH PH·ª¶\"\n",
    "    if not metadata[\"issuing_agency\"]:\n",
    "        scan = content[:800].upper()\n",
    "        for ag in AGENCIES:\n",
    "            if ag in scan:\n",
    "                metadata[\"issuing_agency\"] = ag\n",
    "                break\n",
    "\n",
    "    # 3Ô∏è‚É£ Ng√†y ban h√†nh\n",
    "    date_patterns = [\n",
    "        r'ng√†y\\s+(\\d{1,2})\\s+th√°ng\\s+(\\d{1,2})\\s+nƒÉm\\s+(\\d{4})',\n",
    "        r'(\\d{1,2})[./-](\\d{1,2})[./-](\\d{4})'\n",
    "    ]\n",
    "    for pat in date_patterns:\n",
    "        m = re.search(pat, content, re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                d, mth, y = int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "                metadata[\"issue_date\"] = datetime(y, mth, d).strftime(\"%Y-%m-%d\")\n",
    "                break\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "    # 4Ô∏è‚É£ Ti√™u ƒë·ªÅ\n",
    "    metadata[\"title\"] = title.strip() if title else None\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a09688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4Ô∏è‚É£ Chunking\n",
    "# ===============================\n",
    "def chunk_document(content: str, max_chunk_size: int = 1000, overlap_words: int = 50) -> List[str]:\n",
    "    content = clean_formatting(content)\n",
    "    parts = re.split(r'(?=(ƒêi·ªÅu\\s+th·ª©|KHO·∫¢N\\s+TH·ª®|CH∆Ø∆†NG\\s+|M·ª§C\\s+))', content, flags=re.IGNORECASE)\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', content)\n",
    "        parts = []\n",
    "        current = \"\"\n",
    "        for s in sentences:\n",
    "            if len(current) + len(s) < max_chunk_size:\n",
    "                current += \" \" + s\n",
    "            else:\n",
    "                parts.append(current.strip())\n",
    "                current = s\n",
    "        if current:\n",
    "            parts.append(current.strip())\n",
    "    else:\n",
    "        merged = []\n",
    "        for i in range(0, len(parts), 2):\n",
    "            section = \"\".join(parts[i:i+2]).strip()\n",
    "            if section:\n",
    "                merged.append(section)\n",
    "        parts = merged\n",
    "\n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "    for p in parts:\n",
    "        if len(buffer) + len(p) < max_chunk_size:\n",
    "            buffer += \" \" + p\n",
    "        else:\n",
    "            chunks.append(buffer.strip())\n",
    "            buffer = p\n",
    "    if buffer:\n",
    "        chunks.append(buffer.strip())\n",
    "\n",
    "    if overlap_words > 0 and len(chunks) > 1:\n",
    "        result = [chunks[0]]\n",
    "        for i in range(1, len(chunks)):\n",
    "            prev = chunks[i-1].split()\n",
    "            overlap = \" \".join(prev[-overlap_words:]) if len(prev) > overlap_words else chunks[i-1]\n",
    "            result.append((overlap + \" \" + chunks[i]).strip())\n",
    "        chunks = result\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ===============================\n",
    "# 5Ô∏è‚É£ Chu·∫©n h√≥a b·∫£n ghi\n",
    "# ===============================\n",
    "def normalize_record(doc: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"D·ªØ li·ªáu g·ªëc: content = category, mt6_text = full text\"\"\"\n",
    "    category = doc.get(\"content\") or \"\"\n",
    "    content = doc.get(\"mt6_text\") or \"\"\n",
    "    content = clean_formatting(content)\n",
    "    law_id = str(doc.get(\"law_id\") or doc.get(\"lawid\") or \"unknown\")\n",
    "    return {\n",
    "        \"law_id\": law_id,\n",
    "        \"raw_title\": doc.get(\"title\"),\n",
    "        \"source_url\": doc.get(\"href\"),\n",
    "        \"category\": category.strip(),\n",
    "        \"content\": content,\n",
    "        \"provided_issue_date\": doc.get(\"date\")\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb737953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 6Ô∏è‚É£ Pipeline ch√≠nh\n",
    "# ===============================\n",
    "def process_law_data(input_path: str, max_chunk_size: int = 400, overlap_words: int = 30) -> str:\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y file: {input_path}\")\n",
    "\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    output_dir = os.path.dirname(input_path) or \".\"\n",
    "    processed_path = os.path.join(output_dir, f\"{base}_processedv6.json\")\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_data = json.load(f)\n",
    "    if isinstance(raw_data, dict):\n",
    "        raw_data = [raw_data]\n",
    "\n",
    "    processed: List[Dict[str, Any]] = []\n",
    "\n",
    "    for doc in raw_data:\n",
    "        norm = normalize_record(doc)\n",
    "        metadata = extract_metadata(norm[\"content\"], norm[\"law_id\"], norm.get(\"raw_title\"))\n",
    "        chunks = chunk_document(norm[\"content\"], max_chunk_size=max_chunk_size, overlap_words=overlap_words)\n",
    "\n",
    "        # ‚úÖ ∆Øu ti√™n category t·ª´ d·ªØ li·ªáu g·ªëc\n",
    "        if norm[\"category\"]:\n",
    "            final_category = norm[\"category\"]\n",
    "        else:\n",
    "            final_category = infer_field_category(norm[\"raw_title\"], metadata[\"issuing_agency\"], norm[\"content\"])\n",
    "\n",
    "        for chunk_num, chunk_text in enumerate(chunks, start=1):\n",
    "            record = {\n",
    "                \"law_id\": norm[\"law_id\"],\n",
    "                \"chunk_num\": chunk_num,\n",
    "                \"document_type\": metadata[\"document_type\"],\n",
    "                \"issuing_agency\": metadata[\"issuing_agency\"],\n",
    "                \"issue_date\": metadata[\"issue_date\"] or norm.get(\"provided_issue_date\"),\n",
    "                \"title\": metadata[\"title\"],\n",
    "                \"source_url\": norm[\"source_url\"],\n",
    "                \"raw_title\": norm[\"raw_title\"],\n",
    "                \"category\": final_category,\n",
    "                \"chunk\": chunk_text\n",
    "            }\n",
    "            record = {k: v for k, v in record.items() if v not in [None, \"\"]}\n",
    "            processed.append(record)\n",
    "\n",
    "\n",
    "    with open(processed_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return processed_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dec0b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ x·ª≠ l√Ω xong!\n",
      "üìÑ File ƒë·∫ßu ra: C:\\Code\\Ky5\\SEG\\Crawl\\Dataprocessing\\Process\\tvpl_congvan3days_processedv6.json\n",
      "üì¶ T·ªïng s·ªë record (chunk): 374251\n",
      "\n",
      "--- V√≠ d·ª• record ƒë·∫ßu ti√™n ---\n",
      "{\n",
      "  \"law_id\": \"22714\",\n",
      "  \"chunk_num\": 1,\n",
      "  \"document_type\": \"NGH·ªä ƒê·ªäNH\",\n",
      "  \"issuing_agency\": \"B·ªò TR∆Ø·ªûNG B·ªò QU·ªêC GIA GI√ÅO D·ª§C BAN H√ÄNH\",\n",
      "  \"issue_date\": \"1945-10-15\",\n",
      "  \"title\": \"Ngh·ªã ƒë·ªãnh nƒÉm 1945 v·ªÅ H·ªôi ƒë·ªìng c·ªë v·∫•n h·ªçc ch√≠nh do B·ªô tr∆∞·ªüng B·ªô Qu·ªëc Gia Gi√°o D·ª•c ban h√†nh.\",\n",
      "  \"source_url\": \"https://thuvienphapluat.vn/van-ban/Giao-duc/Nghi-dinh-Hoi-dong-co-van-hoc-chinh-22714.aspx\",\n",
      "  \"raw_title\": \"Ngh·ªã ƒë·ªãnh nƒÉm 1945 v·ªÅ H·ªôi ƒë·ªìng c·ªë v·∫•n h·ªçc ch√≠nh do B·ªô tr∆∞·ªüng B·ªô Qu·ªëc Gia Gi√°o D·ª•c ban h√†nh.\",\n",
      "  \"category\": \"Gi√°o d·ª•c\",\n",
      "  \"chunk\": \"B·ªò QU·ªêC GIA GI√ÅO D·ª§C VI·ªÜT NAM D√ÇN CH·ª¶ C·ªòNG H√íA ƒê·ªôc l·∫≠p - T·ª± do - H·∫°nh ph√∫c S·ªë: KH√îNG S·ªê1 H√† N·ªôi, ng√†y 15 th√°ng 10 nƒÉm 1945 B·ªò TR∆Ø·ªûNG B·ªò QU·ªêC GIA GI√ÅO D·ª§C Chi·∫øu ch·ªâ S·∫Øc l·ªánh s·ªë 44 ng√†y 10 th√°ng 10 nƒÉm 1945 thi·∫øt l·∫≠p H·ªôi ƒë·ªìng c·ªë v·∫•n H·ªçc ch√≠nh. NGH·ªä ƒê·ªäNH: ƒêi·ªÅu th·ª©\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 7Ô∏è‚É£ G·ªçi th·ª≠ pipeline\n",
    "# ===============================\n",
    "input_path = r\"C:\\Code\\Ky5\\SEG\\Crawl\\Dataprocessing\\Process\\tvpl_congvan3days.json\"\n",
    "output_path = process_law_data(input_path)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ x·ª≠ l√Ω xong!\")\n",
    "print(f\"üìÑ File ƒë·∫ßu ra: {output_path}\")\n",
    "\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"üì¶ T·ªïng s·ªë record (chunk): {len(data)}\")\n",
    "print(\"\\n--- V√≠ d·ª• record ƒë·∫ßu ti√™n ---\")\n",
    "print(json.dumps(data[0], ensure_ascii=False, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
